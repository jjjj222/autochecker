--------------------------------------------------
                    Setting
--------------------------------------------------
start_time: 2016/11/20-08:50:35
git_hash: fe117dd6989028d01c3cc343d73d9e3f4f03fbba
conll_file: ../data/all/all.conll
ann_file: ../data/all/all.conll.ann
log_file: ../log/all/3011110110101.log
parameter: 30111101101010000
seed: 0
classifier: maxent
active_features: first-tag head-dprel head-tag head-word next-word next-word-2 prev-tag prev-word
inactive_features: current-det first-word head-parent next-tag next-tag-2 prev-tag-2 prev-word-2
documents_list_0: 1109 2237 2319 975 1743 2153 2453 1289 1218 1271 1537 2587 1587 1136 1242 963 973 924 1785 1564 1552 1022 1098 2648 1413 2192 2506 851 1690 1426 863 2527 2532 2255 983 1641 2461 2161 1122 2185 880 2347 1582 1060 1050 2514 965 1591 2343 2215 1396 1047 2324 1756 1446 2543 1296 1722 1200 1044 1003 1104 910 1096 1130 2433 1045 2165 1232 1028 1589 1423 952 1593 836 2359 1391 1246 2242 1148 1018 1750 1209 2636 2222 1759 1555 2553 1619 2519 1513 1215 1682 2231 1625 2538 2334 2364 1678 895 1655 1677 2489 2468 1189 1790 986 1614 1597 901 1633 1286 1159 839 1719 2208 1299 2168 2593 1767 1707 2621 2563 2239 1146 1783 1113 2436 1736 2233 2240 2276 2205 1657 977 1092 2184 1457 2471 2303
documents_list_1: 1505 926 2354 1639 1315 2189 2383 1747 1586 1702 2147 2590 1691 2501 2552 1602 1305 1304 1089 1049 2517 1496 1502 1550 1244 1519 1404 1769 2614 1292 887 2304 1314 1180 1764 1071 2649 2352 1063 1309 1717 2424 1288 2193 1010 1481 1798 2562 2155 2413 1024 1336 1219 2546 1506 1420 2440 2472 1728 1274 980 1013 1139 2355 2336 962 1387 2372 1592 2250 1107 2420 2365 1293 1521 1383 950 1323 1070 2373 1795 1729 1035 1654 2409 2300 2634 1088 2329 892 1732 1718 2149 900 1503 2573 932 2399 2289 1085 2415 2307 1727 2535 1059 1578 1061 1793 1494 1560 1388 909 2314 1526 877 1236 2197 831 2395 2583 1322 1652 2378 2578 1263 1287 1163 857 1547 1119 2407 1196 949 1454 2466 2390 848 969 1181 1425
documents_list_2: 1721 882 964 1500 1568 1766 2392 2348 1415 1386 1604 1773 2309 1006 1008 2537 2291 2145 1120 1787 1118 1478 2159 1624 2218 1684 1079 1179 967 1123 1329 1093 1643 1726 1152 2331 1036 1038 1559 903 2411 1429 1228 1453 1438 2142 845 2282 1233 1033 1212 1658 1117 1224 1699 1126 1558 904 1230 2404 2473 954 1779 1715 1462 953 2203 2446 2254 1683 2183 1511 1168 860 1185 1193 1318 1282 1482 1255 1405 1430 2596 1229 2173 846 2493 1250 2361 1594 1694 2160 917 2499 1686 2568 1468 978 956 1364 1692 2406 2442 1735 1497 2298 2512 898 1390 2277 1216 1444 1016 1067 2248 1613 947 2618 1054 1443 1337 1341 1128 1221 2262 1082 959 876 2381 982 2515 1794 1492 1647 1545 1279 1610 1510 1720 1575
documents_list_3: 2223 1261 2265 1081 2462 1264 1533 2377 1544 1170 2344 2576 1437 2441 946 2221 933 1197 2531 2148 2225 1607 2550 1027 2357 2293 2236 1095 868 1101 2380 1267 1312 2591 1486 1019 2292 1331 1025 1129 2199 832 2306 1669 2154 1627 1270 2310 1243 2219 1392 2495 1664 849 2188 1300 1753 843 1527 1220 2179 2176 854 881 2479 2438 897 2525 1515 1801 1249 2302 907 833 2592 1147 2507 2191 1771 1685 2315 2425 2569 1077 2639 943 966 1151 2485 2496 1774 1378 1031 2523 1730 1284 2610 1518 2602 1012 2445 1370 871 2350 1042 987 874 2544 1325 1247 2290 1032 2463 1472 2285 1463 2351 1083 2166 1491 2497 2490 1178 1489 1706 2454 1398 2603 1599 2555 1725 1375 1734 1668 1268 1165 2311 2144 2258 1195
documents_list_4: 1069 2271 1581 1646 2384 1432 1588 970 2313 1097 1001 1571 2194 1258 1442 2398 1187 2650 2448 1536 1411 1539 2426 937 1141 1762 2571 1183 2449 1576 2201 1303 2178 1579 1534 834 1421 1548 872 1458 2175 1741 1796 2226 1140 2601 1781 867 2318 1746 2518 1436 991 2169 1380 2214 1638 1171 2167 2459 1313 1007 1379 906 2321 968 1057 1751 1517 2316 1524 2211 1723 1330 2207 1191 1651 1056 1713 862 2494 996 2620 2342 2615 869 2427 1710 835 2299 1483 1226 1389 1570 2430 2526 861 2630 891 829 1399 1782 1062 1100 1563 2246 1052 1302 2645 1549 1797 883 1561 1260 2296 985 1161 1208 2245 2551 2266 2540 885 1565 902 957 858 1310 888 1149 1449 1573 1675 2502 939 2389 1190 2450 1227 1448
documents_list_5: 2580 2340 935 955 2412 2317 1332 1656 1173 2264 2349 1611 894 2345 1240 1115 1108 1671 1198 2477 915 908 1596 1688 913 1508 1384 1417 1238 2369 2511 2475 1030 1068 2444 912 1636 2346 890 1621 2443 1770 2417 1086 948 2335 1335 1530 1650 1693 920 1788 1188 1037 1105 1339 1276 2408 2613 1373 1708 1546 2577 1127 2533 2339 1742 859 1554 1711 2498 1368 1406 1407 2353 2328 1465 2156 1577 1776 2187 1231 1203 1103 2202 1194 919 2382 940 2455 2422 934 1479 1254 2229 2180 2557 2414 1662 1516 1039 1374 870 984 2625 2256 1262 2604 2617 2212 2536 2482 1217 1590 1501 2200 2224 1661 2164 1145 1295 1618 1248 1367 2418 879 2400 1338 1765 2308 1620 2429 1080 1584 1464 1531 2396 2209 1562 2270
documents_list_6: 942 2483 2327 1712 2492 1687 1412 2500 1073 2232 1484 1569 1623 1021 1439 2275 1273 1169 2274 1583 1433 2367 1365 1630 1763 981 1402 2244 2235 1567 2431 2478 1174 1724 2457 2338 2247 1210 1087 2140 2371 2541 1269 1132 1761 2530 1316 1400 1617 1176 1473 853 2368 2638 2528 1605 2574 1154 2337 1142 971 1700 1789 1485 2267 1538 1476 1011 1272 1283 2272 2362 1744 1745 1418 1366 1459 958 1456 927 1278 2261 1520 1471 2509 2467 2520 2386 1697 856 2486 1114 1740 2554 951 2312 1294 1319 1239 1659 1780 2190 1760 866 1382 990 2469 1116 837 2186 2210 2410 2635 2234 1645 1784 2522 1311 1416 1394 1632 2524 1777 1445 1703 1495 2220 1598 1020 1681 2456 2600 1084 2301 2294 830 1514 1265 2474 2393
documents_list_7: 2435 2503 1553 2143 2278 2434 1786 2556 1000 2465 972 2439 1395 2516 2451 1466 2452 2251 1131 1626 2158 999 2581 2162 944 1158 1542 1133 1376 1572 976 1477 1204 1004 1223 1051 2402 1422 2195 1023 2491 1792 1253 2230 2623 993 1320 2341 2387 1201 1053 841 1182 2332 1739 1164 2216 1307 2213 1635 989 1029 1704 1377 1419 847 1099 2170 1150 1608 1110 1616 2295 2358 2150 936 1144 2403 1705 1649 1369 1778 1431 1184 1490 1509 2268 1137 1460 1157 1528 2356 2287 1009 2394 1606 875 1245 1134 2460 1615 1529 918 1714 1172 1440 1585 2305 896 1670 941 2476 1758 1674 2488 1385 2611 2375 1207 886 1124 2366 844 1048 1043 1058 1102 1637 960 1493 1634 2323 1222 1595 2521 1775 2539 2458 1673
documents_list_8: 2416 2182 855 2280 1757 1709 2505 2198 1435 2534 2627 1015 2447 1291 1461 2594 1755 2374 1090 2437 1731 1666 1324 1162 2480 974 1535 1334 1672 2257 2238 1397 914 1175 1005 2177 2428 1297 1612 2513 1285 979 1257 2589 1427 2566 1487 1653 1557 1644 2320 1160 2174 1667 1424 2283 873 930 1251 1543 1749 2269 1112 2141 1034 1046 2646 1317 1076 961 1434 1507 1371 2470 2172 2561 1327 1026 1791 1055 1772 2391 1301 1177 1306 1040 1414 2651 1014 893 1566 2146 1290 1600 1452 1202 1211 865 1800 1447 2204 2508 2284 1802 2397 1237 1321 1234 1680 1166 1298 1475 1213 1609 850 1153 945 2624 1450 2547 1241 1275 1551 1410 2360 1280 988 1676 2263 1660 2385 2432 1628 2151 1156 2333 2464 2330 1523
documents_list_9: 2297 2322 1574 1522 992 1075 2363 922 1488 2181 928 2622 1648 2326 1333 1451 2152 1504 1512 1498 1469 916 2549 1078 2529 2484 2379 842 1642 1214 2260 1474 2163 2196 1408 1428 1622 931 2401 2388 1256 1121 1540 838 2542 899 2548 1393 1525 1768 2405 840 1065 1470 2241 1556 1125 997 1235 2157 1689 1401 1441 2286 1541 1135 2281 2279 1467 889 1111 1138 1499 2481 1328 2421 1738 1737 911 1665 1074 1631 2273 878 1192 1106 1698 995 1017 1799 921 1580 1733 1326 2612 1167 1455 884 1716 1225 2545 994 1695 2206 1381 1041 2171 2419 923 1308 1199 1281 1002 1663 1155 925 1480 929 2504 1340 1064 1186 1143 1603 2259 1679 1066 864 1266 938 2325 2370 1372 1601 1629 1403 1701 2585 1259
--------------------------------------------------
                    Round 0
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.48981        0.688
             3          -0.43650        0.778
             4          -0.40003        0.813
             5          -0.37425        0.827
             6          -0.35506        0.835
             7          -0.34018        0.840
             8          -0.32827        0.844
             9          -0.31848        0.847
            10          -0.31029        0.849
            11          -0.30331        0.852
            12          -0.29729        0.854
            13          -0.29203        0.855
            14          -0.28739        0.857
            15          -0.28326        0.858
            16          -0.27956        0.859
            17          -0.27622        0.860
            18          -0.27319        0.860
            19          -0.27042        0.861
            20          -0.26788        0.862
            21          -0.26554        0.862
            22          -0.26338        0.863
            23          -0.26137        0.864
            24          -0.25951        0.864
            25          -0.25776        0.864
            26          -0.25613        0.865
            27          -0.25460        0.865
            28          -0.25316        0.866
            29          -0.25180        0.866
            30          -0.25051        0.866
            31          -0.24929        0.866
            32          -0.24814        0.867
            33          -0.24704        0.867
            34          -0.24600        0.867
            35          -0.24501        0.867
            36          -0.24406        0.867
            37          -0.24315        0.868
            38          -0.24229        0.868
            39          -0.24146        0.868
            40          -0.24066        0.868
            41          -0.23990        0.869
            42          -0.23916        0.869
            43          -0.23846        0.869
            44          -0.23778        0.869
            45          -0.23712        0.869
            46          -0.23649        0.869
            47          -0.23588        0.869
            48          -0.23529        0.870
            49          -0.23472        0.870
            50          -0.23417        0.870
            51          -0.23364        0.870
            52          -0.23313        0.870
            53          -0.23263        0.870
            54          -0.23214        0.870
            55          -0.23167        0.870
            56          -0.23122        0.870
            57          -0.23078        0.871
            58          -0.23035        0.871
            59          -0.22993        0.871
            60          -0.22952        0.871
            61          -0.22913        0.871
            62          -0.22874        0.871
            63          -0.22837        0.871
            64          -0.22800        0.871
            65          -0.22765        0.871
            66          -0.22730        0.872
            67          -0.22696        0.872
            68          -0.22663        0.872
            69          -0.22631        0.872
            70          -0.22600        0.872
            71          -0.22569        0.872
            72          -0.22539        0.872
            73          -0.22510        0.872
            74          -0.22482        0.872
            75          -0.22454        0.872
            76          -0.22426        0.872
            77          -0.22399        0.872
            78          -0.22373        0.872
            79          -0.22348        0.872
            80          -0.22323        0.873
            81          -0.22298        0.873
            82          -0.22274        0.873
            83          -0.22251        0.873
            84          -0.22227        0.873
            85          -0.22205        0.873
            86          -0.22183        0.873
            87          -0.22161        0.873
            88          -0.22140        0.873
            89          -0.22119        0.873
            90          -0.22098        0.873
            91          -0.22078        0.873
            92          -0.22058        0.873
            93          -0.22039        0.873
            94          -0.22020        0.873
            95          -0.22001        0.873
            96          -0.21983        0.873
            97          -0.21965        0.873
            98          -0.21947        0.873
            99          -0.21929        0.874
         Final          -0.21912        0.874
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147482>  1366   8166 |
  a |   2391 <10722>  3460 |
the |  12644   1604 <46444>|
----+----------------------+
(row = reference; col = test)

0_train_classifier_accuracy: 0.873522594855
0_train_correction: 31899
0_train_golden_correction: 4913
0_train_tp: 3172
0_train_fp: 28727
0_train_fn: 1741
0_train_precision: 0.0994388538826
0_train_recall: 0.64563403216
0_train_f1: 0.172335108117

  17.380 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  16.757 next-word-2=='barker' and label is 'the'
  15.701 head-word=='distance13' and label is 'a'
  14.993 head-word=='approach*' and label is 'a'
  14.790 head-word=='crossroads' and label is 'a'
 -14.491 first-tag=='PRP' and label is 'the'
  14.405 head-word=='metres' and label is 'a'
  14.067 head-word=='35km/h' and label is 'a'
  13.739 next-word-2=='//www.rlhleagueofnurses.org.uk/hospital_history/medical_developments/me' and label is 'the'
  13.143 head-word=='radius' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15767>  254  1309 |
  a |   362  <852>  522 |
the |  1849   263 <4394>|
----+-------------------+
(row = reference; col = test)

0_test_classifier_accuracy: 0.82171906773
0_test_correction: 4720
0_test_golden_correction: 503
0_test_tp: 280
0_test_fp: 4440
0_test_fn: 223
0_test_precision: 0.0593220338983
0_test_recall: 0.556660039761
0_test_f1: 0.107218073904

--------------------------------------------------
                    Round 1
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.257
             2          -0.48540        0.690
             3          -0.43298        0.779
             4          -0.39695        0.814
             5          -0.37143        0.828
             6          -0.35242        0.836
             7          -0.33767        0.841
             8          -0.32585        0.845
             9          -0.31616        0.848
            10          -0.30803        0.851
            11          -0.30111        0.853
            12          -0.29514        0.855
            13          -0.28993        0.856
            14          -0.28532        0.858
            15          -0.28123        0.859
            16          -0.27756        0.860
            17          -0.27425        0.861
            18          -0.27124        0.862
            19          -0.26850        0.862
            20          -0.26598        0.863
            21          -0.26366        0.863
            22          -0.26152        0.864
            23          -0.25953        0.864
            24          -0.25767        0.865
            25          -0.25595        0.865
            26          -0.25433        0.866
            27          -0.25281        0.866
            28          -0.25138        0.866
            29          -0.25003        0.867
            30          -0.24875        0.867
            31          -0.24755        0.867
            32          -0.24640        0.868
            33          -0.24531        0.868
            34          -0.24428        0.868
            35          -0.24329        0.868
            36          -0.24235        0.869
            37          -0.24146        0.869
            38          -0.24060        0.869
            39          -0.23977        0.869
            40          -0.23898        0.869
            41          -0.23823        0.870
            42          -0.23750        0.870
            43          -0.23680        0.870
            44          -0.23612        0.870
            45          -0.23547        0.870
            46          -0.23485        0.870
            47          -0.23425        0.870
            48          -0.23366        0.871
            49          -0.23310        0.871
            50          -0.23255        0.871
            51          -0.23203        0.871
            52          -0.23152        0.871
            53          -0.23102        0.871
            54          -0.23054        0.871
            55          -0.23008        0.872
            56          -0.22963        0.872
            57          -0.22919        0.872
            58          -0.22876        0.872
            59          -0.22835        0.872
            60          -0.22795        0.872
            61          -0.22756        0.872
            62          -0.22718        0.872
            63          -0.22681        0.872
            64          -0.22644        0.872
            65          -0.22609        0.872
            66          -0.22575        0.872
            67          -0.22542        0.873
            68          -0.22509        0.873
            69          -0.22477        0.873
            70          -0.22446        0.873
            71          -0.22416        0.873
            72          -0.22386        0.873
            73          -0.22357        0.873
            74          -0.22329        0.873
            75          -0.22301        0.873
            76          -0.22274        0.873
            77          -0.22248        0.873
            78          -0.22222        0.873
            79          -0.22197        0.873
            80          -0.22172        0.873
            81          -0.22148        0.873
            82          -0.22124        0.873
            83          -0.22101        0.873
            84          -0.22078        0.873
            85          -0.22055        0.873
            86          -0.22034        0.873
            87          -0.22012        0.874
            88          -0.21991        0.874
            89          -0.21970        0.874
            90          -0.21950        0.874
            91          -0.21930        0.874
            92          -0.21911        0.874
            93          -0.21891        0.874
            94          -0.21873        0.874
            95          -0.21854        0.874
            96          -0.21836        0.874
            97          -0.21818        0.874
            98          -0.21801        0.874
            99          -0.21783        0.874
         Final          -0.21766        0.874
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147829>  1368   8123 |
  a |   2390 <10487>  3409 |
the |  12564   1569 <45811>|
----+----------------------+
(row = reference; col = test)

1_train_classifier_accuracy: 0.874018411475
1_train_correction: 31670
1_train_golden_correction: 4785
1_train_tp: 3118
1_train_fp: 28552
1_train_fn: 1667
1_train_precision: 0.0984527944427
1_train_recall: 0.651619644723
1_train_f1: 0.171060211219

  17.654 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  15.581 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  15.372 head-word=='distance13' and label is 'a'
  15.321 head-word=='approach*' and label is 'a'
 -14.824 first-tag=='PRP' and label is 'the'
  14.695 head-word=='cattles' and label is 'a'
  14.402 head-word=='crossroads' and label is 'a'
  14.374 head-word=='metres' and label is 'a'
  14.060 head-word=='35km/h' and label is 'a'
  13.694 next-word-2=='//www.rlhleagueofnurses.org.uk/hospital_history/medical_developments/me' and label is 'the'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15429>  271  1324 |
  a |   407  <974>  642 |
the |  2034   302 <4918>|
----+-------------------+
(row = reference; col = test)

1_test_classifier_accuracy: 0.810653587316
1_test_correction: 5155
1_test_golden_correction: 631
1_test_tp: 330
1_test_fp: 4825
1_test_fn: 301
1_test_precision: 0.0640155189137
1_test_recall: 0.522979397781
1_test_f1: 0.114068441065

--------------------------------------------------
                    Round 2
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.258
             2          -0.48807        0.689
             3          -0.43545        0.779
             4          -0.39931        0.813
             5          -0.37371        0.827
             6          -0.35461        0.835
             7          -0.33978        0.841
             8          -0.32789        0.844
             9          -0.31813        0.847
            10          -0.30994        0.850
            11          -0.30297        0.852
            12          -0.29694        0.854
            13          -0.29168        0.855
            14          -0.28704        0.857
            15          -0.28291        0.858
            16          -0.27921        0.859
            17          -0.27586        0.860
            18          -0.27283        0.861
            19          -0.27006        0.862
            20          -0.26752        0.862
            21          -0.26518        0.863
            22          -0.26301        0.863
            23          -0.26100        0.864
            24          -0.25913        0.864
            25          -0.25739        0.865
            26          -0.25575        0.865
            27          -0.25422        0.865
            28          -0.25278        0.866
            29          -0.25141        0.866
            30          -0.25013        0.866
            31          -0.24891        0.867
            32          -0.24776        0.867
            33          -0.24666        0.867
            34          -0.24561        0.867
            35          -0.24462        0.868
            36          -0.24367        0.868
            37          -0.24276        0.868
            38          -0.24190        0.868
            39          -0.24106        0.869
            40          -0.24027        0.869
            41          -0.23950        0.869
            42          -0.23877        0.869
            43          -0.23806        0.869
            44          -0.23738        0.870
            45          -0.23672        0.870
            46          -0.23609        0.870
            47          -0.23548        0.870
            48          -0.23489        0.870
            49          -0.23432        0.870
            50          -0.23377        0.870
            51          -0.23324        0.871
            52          -0.23272        0.871
            53          -0.23222        0.871
            54          -0.23174        0.871
            55          -0.23127        0.871
            56          -0.23081        0.871
            57          -0.23037        0.871
            58          -0.22994        0.871
            59          -0.22952        0.871
            60          -0.22912        0.872
            61          -0.22872        0.872
            62          -0.22833        0.872
            63          -0.22796        0.872
            64          -0.22759        0.872
            65          -0.22724        0.872
            66          -0.22689        0.872
            67          -0.22655        0.872
            68          -0.22622        0.872
            69          -0.22590        0.872
            70          -0.22559        0.872
            71          -0.22528        0.872
            72          -0.22498        0.872
            73          -0.22469        0.873
            74          -0.22440        0.873
            75          -0.22412        0.873
            76          -0.22385        0.873
            77          -0.22358        0.873
            78          -0.22332        0.873
            79          -0.22306        0.873
            80          -0.22281        0.873
            81          -0.22256        0.873
            82          -0.22232        0.873
            83          -0.22208        0.873
            84          -0.22185        0.873
            85          -0.22163        0.873
            86          -0.22140        0.873
            87          -0.22119        0.873
            88          -0.22097        0.873
            89          -0.22076        0.874
            90          -0.22056        0.874
            91          -0.22036        0.874
            92          -0.22016        0.874
            93          -0.21996        0.874
            94          -0.21977        0.874
            95          -0.21958        0.874
            96          -0.21940        0.874
            97          -0.21922        0.874
            98          -0.21904        0.874
            99          -0.21886        0.874
         Final          -0.21869        0.874
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147894>  1373   8126 |
  a |   2417 <10647>  3470 |
the |  12586   1578 <46172>|
----+----------------------+
(row = reference; col = test)

2_train_classifier_accuracy: 0.873859721766
2_train_correction: 31828
2_train_golden_correction: 4807
2_train_tp: 3126
2_train_fp: 28702
2_train_fn: 1681
2_train_precision: 0.098215407817
2_train_recall: 0.650301643437
2_train_f1: 0.170656476047

  17.618 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  17.282 next-word-2=='barker' and label is 'the'
  15.495 head-word=='distance13' and label is 'a'
  15.456 head-word=='approach*' and label is 'a'
  15.254 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
 -14.714 first-tag=='PRP' and label is 'the'
  14.710 head-word=='cattles' and label is 'a'
  14.564 head-word=='crossroads' and label is 'a'
  14.433 head-word=='metres' and label is 'a'
  14.041 head-word=='35km/h' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15456>  253  1242 |
  a |   420  <848>  507 |
the |  2029   287 <4546>|
----+-------------------+
(row = reference; col = test)

2_test_classifier_accuracy: 0.814835078943
2_test_correction: 4888
2_test_golden_correction: 609
2_test_tp: 333
2_test_fp: 4555
2_test_fn: 276
2_test_precision: 0.0681260229133
2_test_recall: 0.546798029557
2_test_f1: 0.121156994724

--------------------------------------------------
                    Round 3
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.49073        0.688
             3          -0.43747        0.778
             4          -0.40108        0.813
             5          -0.37536        0.827
             6          -0.35621        0.835
             7          -0.34135        0.840
             8          -0.32945        0.844
             9          -0.31968        0.847
            10          -0.31150        0.849
            11          -0.30453        0.852
            12          -0.29851        0.853
            13          -0.29326        0.855
            14          -0.28862        0.856
            15          -0.28449        0.857
            16          -0.28079        0.859
            17          -0.27745        0.860
            18          -0.27442        0.860
            19          -0.27165        0.861
            20          -0.26911        0.862
            21          -0.26676        0.862
            22          -0.26460        0.863
            23          -0.26259        0.863
            24          -0.26072        0.864
            25          -0.25897        0.864
            26          -0.25733        0.864
            27          -0.25579        0.865
            28          -0.25435        0.865
            29          -0.25298        0.866
            30          -0.25169        0.866
            31          -0.25047        0.866
            32          -0.24931        0.866
            33          -0.24821        0.867
            34          -0.24716        0.867
            35          -0.24616        0.867
            36          -0.24521        0.867
            37          -0.24430        0.868
            38          -0.24343        0.868
            39          -0.24259        0.868
            40          -0.24179        0.868
            41          -0.24102        0.869
            42          -0.24028        0.869
            43          -0.23957        0.869
            44          -0.23888        0.869
            45          -0.23822        0.869
            46          -0.23759        0.869
            47          -0.23697        0.870
            48          -0.23638        0.870
            49          -0.23581        0.870
            50          -0.23525        0.870
            51          -0.23472        0.870
            52          -0.23420        0.870
            53          -0.23369        0.870
            54          -0.23320        0.870
            55          -0.23273        0.870
            56          -0.23227        0.871
            57          -0.23182        0.871
            58          -0.23139        0.871
            59          -0.23097        0.871
            60          -0.23056        0.871
            61          -0.23016        0.871
            62          -0.22977        0.871
            63          -0.22939        0.871
            64          -0.22902        0.871
            65          -0.22866        0.871
            66          -0.22831        0.871
            67          -0.22797        0.872
            68          -0.22764        0.872
            69          -0.22731        0.872
            70          -0.22700        0.872
            71          -0.22669        0.872
            72          -0.22638        0.872
            73          -0.22609        0.872
            74          -0.22580        0.872
            75          -0.22552        0.872
            76          -0.22524        0.872
            77          -0.22497        0.872
            78          -0.22470        0.872
            79          -0.22444        0.872
            80          -0.22419        0.872
            81          -0.22394        0.872
            82          -0.22370        0.872
            83          -0.22346        0.872
            84          -0.22323        0.872
            85          -0.22300        0.873
            86          -0.22277        0.873
            87          -0.22255        0.873
            88          -0.22234        0.873
            89          -0.22212        0.873
            90          -0.22191        0.873
            91          -0.22171        0.873
            92          -0.22151        0.873
            93          -0.22131        0.873
            94          -0.22112        0.873
            95          -0.22093        0.873
            96          -0.22074        0.873
            97          -0.22056        0.873
            98          -0.22038        0.873
            99          -0.22020        0.873
         Final          -0.22003        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<146950>  1368   8115 |
  a |   2355 <10606>  3436 |
the |  12745   1578 <46202>|
----+----------------------+
(row = reference; col = test)

3_train_classifier_accuracy: 0.87316749159
3_train_correction: 31852
3_train_golden_correction: 4823
3_train_tp: 3138
3_train_fp: 28714
3_train_fn: 1685
3_train_precision: 0.0985181464272
3_train_recall: 0.650632386481
3_train_f1: 0.171124744376

  17.481 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  17.079 next-word-2=='barker' and label is 'the'
  15.426 head-word=='distance13' and label is 'a'
  15.302 head-word=='approach*' and label is 'a'
  15.218 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  14.605 head-word=='cattles' and label is 'a'
  14.471 head-word=='crossroads' and label is 'a'
 -14.467 first-tag=='PRP' and label is 'the'
  14.345 head-word=='metres' and label is 'a'
  14.320 prev-word=='conceptualize' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<16335>  293  1283 |
  a |   379  <935>  598 |
the |  1899   276 <4498>|
----+-------------------+
(row = reference; col = test)

3_test_classifier_accuracy: 0.821557971014
3_test_correction: 4897
3_test_golden_correction: 593
3_test_tp: 322
3_test_fp: 4575
3_test_fn: 271
3_test_precision: 0.0657545435981
3_test_recall: 0.543001686341
3_test_f1: 0.117304189435

--------------------------------------------------
                    Round 4
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.49004        0.688
             3          -0.43679        0.779
             4          -0.40033        0.813
             5          -0.37455        0.827
             6          -0.35536        0.835
             7          -0.34048        0.840
             8          -0.32856        0.844
             9          -0.31878        0.847
            10          -0.31058        0.849
            11          -0.30361        0.852
            12          -0.29758        0.853
            13          -0.29232        0.855
            14          -0.28768        0.856
            15          -0.28355        0.858
            16          -0.27985        0.859
            17          -0.27651        0.860
            18          -0.27348        0.861
            19          -0.27071        0.861
            20          -0.26817        0.862
            21          -0.26583        0.862
            22          -0.26366        0.863
            23          -0.26166        0.863
            24          -0.25979        0.864
            25          -0.25804        0.864
            26          -0.25641        0.865
            27          -0.25487        0.865
            28          -0.25343        0.865
            29          -0.25207        0.866
            30          -0.25078        0.866
            31          -0.24956        0.866
            32          -0.24841        0.866
            33          -0.24731        0.867
            34          -0.24626        0.867
            35          -0.24527        0.867
            36          -0.24432        0.867
            37          -0.24341        0.868
            38          -0.24254        0.868
            39          -0.24171        0.868
            40          -0.24091        0.868
            41          -0.24014        0.868
            42          -0.23941        0.869
            43          -0.23870        0.869
            44          -0.23802        0.869
            45          -0.23736        0.869
            46          -0.23673        0.869
            47          -0.23612        0.869
            48          -0.23553        0.870
            49          -0.23496        0.870
            50          -0.23440        0.870
            51          -0.23387        0.870
            52          -0.23335        0.870
            53          -0.23285        0.870
            54          -0.23237        0.870
            55          -0.23190        0.871
            56          -0.23144        0.871
            57          -0.23099        0.871
            58          -0.23056        0.871
            59          -0.23014        0.871
            60          -0.22974        0.871
            61          -0.22934        0.871
            62          -0.22895        0.871
            63          -0.22858        0.871
            64          -0.22821        0.871
            65          -0.22785        0.871
            66          -0.22751        0.871
            67          -0.22717        0.872
            68          -0.22684        0.872
            69          -0.22651        0.872
            70          -0.22620        0.872
            71          -0.22589        0.872
            72          -0.22559        0.872
            73          -0.22530        0.872
            74          -0.22501        0.872
            75          -0.22473        0.872
            76          -0.22445        0.872
            77          -0.22419        0.872
            78          -0.22392        0.872
            79          -0.22367        0.872
            80          -0.22341        0.872
            81          -0.22317        0.872
            82          -0.22293        0.872
            83          -0.22269        0.872
            84          -0.22246        0.872
            85          -0.22223        0.873
            86          -0.22201        0.873
            87          -0.22179        0.873
            88          -0.22157        0.873
            89          -0.22136        0.873
            90          -0.22116        0.873
            91          -0.22096        0.873
            92          -0.22076        0.873
            93          -0.22056        0.873
            94          -0.22037        0.873
            95          -0.22018        0.873
            96          -0.22000        0.873
            97          -0.21982        0.873
            98          -0.21964        0.873
            99          -0.21946        0.873
         Final          -0.21929        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147485>  1383   8177 |
  a |   2419 <10609>  3504 |
the |  12629   1600 <46450>|
----+----------------------+
(row = reference; col = test)

4_train_classifier_accuracy: 0.873164401339
4_train_correction: 32008
4_train_golden_correction: 4941
4_train_tp: 3193
4_train_fp: 28815
4_train_fn: 1748
4_train_precision: 0.0997563109223
4_train_recall: 0.646225460433
4_train_f1: 0.172832823622

  17.574 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  16.890 next-word-2=='barker' and label is 'the'
  15.343 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  15.269 head-word=='distance13' and label is 'a'
  14.906 head-word=='cattles' and label is 'a'
 -14.492 first-tag=='PRP' and label is 'the'
  14.484 head-word=='metres' and label is 'a'
  14.318 head-word=='crossroads' and label is 'a'
  14.260 head-word=='35km/h' and label is 'a'
  13.315 next-word-2=='//www.rlhleagueofnurses.org.uk/hospital_history/medical_developments/me' and label is 'the'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15722>  241  1336 |
  a |   395  <886>  496 |
the |  1837   295 <4387>|
----+-------------------+
(row = reference; col = test)

4_test_classifier_accuracy: 0.820277397929
4_test_correction: 4746
4_test_golden_correction: 475
4_test_tp: 271
4_test_fp: 4475
4_test_fn: 204
4_test_precision: 0.0571007163928
4_test_recall: 0.570526315789
4_test_f1: 0.103811530358

--------------------------------------------------
                    Round 5
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.48910        0.689
             3          -0.43604        0.779
             4          -0.39977        0.813
             5          -0.37413        0.827
             6          -0.35503        0.835
             7          -0.34021        0.840
             8          -0.32834        0.844
             9          -0.31859        0.847
            10          -0.31042        0.850
            11          -0.30345        0.852
            12          -0.29744        0.854
            13          -0.29219        0.856
            14          -0.28755        0.857
            15          -0.28343        0.858
            16          -0.27973        0.859
            17          -0.27638        0.860
            18          -0.27335        0.861
            19          -0.27058        0.861
            20          -0.26804        0.862
            21          -0.26570        0.862
            22          -0.26353        0.863
            23          -0.26152        0.864
            24          -0.25965        0.864
            25          -0.25791        0.864
            26          -0.25627        0.865
            27          -0.25473        0.865
            28          -0.25329        0.866
            29          -0.25192        0.866
            30          -0.25063        0.866
            31          -0.24941        0.867
            32          -0.24825        0.867
            33          -0.24715        0.867
            34          -0.24611        0.867
            35          -0.24511        0.868
            36          -0.24416        0.868
            37          -0.24325        0.868
            38          -0.24238        0.868
            39          -0.24154        0.868
            40          -0.24074        0.869
            41          -0.23998        0.869
            42          -0.23924        0.869
            43          -0.23853        0.869
            44          -0.23785        0.869
            45          -0.23719        0.869
            46          -0.23655        0.870
            47          -0.23594        0.870
            48          -0.23535        0.870
            49          -0.23478        0.870
            50          -0.23423        0.870
            51          -0.23369        0.870
            52          -0.23317        0.870
            53          -0.23267        0.871
            54          -0.23218        0.871
            55          -0.23171        0.871
            56          -0.23125        0.871
            57          -0.23081        0.871
            58          -0.23038        0.871
            59          -0.22996        0.871
            60          -0.22955        0.871
            61          -0.22915        0.872
            62          -0.22876        0.872
            63          -0.22839        0.872
            64          -0.22802        0.872
            65          -0.22766        0.872
            66          -0.22731        0.872
            67          -0.22697        0.872
            68          -0.22664        0.872
            69          -0.22632        0.872
            70          -0.22600        0.872
            71          -0.22569        0.872
            72          -0.22539        0.872
            73          -0.22510        0.872
            74          -0.22481        0.873
            75          -0.22453        0.873
            76          -0.22425        0.873
            77          -0.22398        0.873
            78          -0.22372        0.873
            79          -0.22346        0.873
            80          -0.22321        0.873
            81          -0.22296        0.873
            82          -0.22272        0.873
            83          -0.22248        0.873
            84          -0.22225        0.873
            85          -0.22202        0.873
            86          -0.22180        0.873
            87          -0.22158        0.873
            88          -0.22136        0.873
            89          -0.22115        0.873
            90          -0.22094        0.873
            91          -0.22074        0.873
            92          -0.22054        0.874
            93          -0.22035        0.874
            94          -0.22015        0.874
            95          -0.21996        0.874
            96          -0.21978        0.874
            97          -0.21960        0.874
            98          -0.21942        0.874
            99          -0.21924        0.874
         Final          -0.21907        0.874
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147025>  1350   8055 |
  a |   2393 <10581>  3484 |
the |  12601   1551 <46138>|
----+----------------------+
(row = reference; col = test)

5_train_classifier_accuracy: 0.873770252768
5_train_correction: 31675
5_train_golden_correction: 4854
5_train_tp: 3132
5_train_fp: 28543
5_train_fn: 1722
5_train_precision: 0.0988792423047
5_train_recall: 0.645241038319
5_train_f1: 0.171480193819

  17.526 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  16.963 next-word-2=='barker' and label is 'the'
  15.395 head-word=='distance13' and label is 'a'
  15.387 next-word-2=='//www.ornl.gov/~webworks/cppr/y2001/pres/120507.pdf' and label is 'the'
  15.207 head-word=='approach*' and label is 'a'
  15.188 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  14.661 head-word=='cattles' and label is 'a'
  14.645 head-word=='foetus' and label is 'a'
 -14.523 first-tag=='PRP' and label is 'the'
  14.482 head-word=='metres' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<16293>  282  1339 |
  a |   393  <942>  516 |
the |  1943   300 <4665>|
----+-------------------+
(row = reference; col = test)

5_test_classifier_accuracy: 0.821054999438
5_test_correction: 4950
5_test_golden_correction: 562
5_test_tp: 320
5_test_fp: 4630
5_test_fn: 242
5_test_precision: 0.0646464646465
5_test_recall: 0.569395017794
5_test_f1: 0.11611030479

--------------------------------------------------
                    Round 6
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.260
             2          -0.49052        0.688
             3          -0.43725        0.779
             4          -0.40080        0.813
             5          -0.37501        0.827
             6          -0.35581        0.835
             7          -0.34091        0.840
             8          -0.32898        0.844
             9          -0.31919        0.847
            10          -0.31099        0.849
            11          -0.30400        0.852
            12          -0.29798        0.854
            13          -0.29271        0.855
            14          -0.28807        0.856
            15          -0.28393        0.858
            16          -0.28023        0.859
            17          -0.27688        0.860
            18          -0.27385        0.860
            19          -0.27107        0.861
            20          -0.26853        0.861
            21          -0.26619        0.862
            22          -0.26402        0.863
            23          -0.26201        0.863
            24          -0.26014        0.863
            25          -0.25839        0.864
            26          -0.25675        0.864
            27          -0.25522        0.865
            28          -0.25377        0.865
            29          -0.25241        0.865
            30          -0.25112        0.866
            31          -0.24990        0.866
            32          -0.24874        0.866
            33          -0.24764        0.867
            34          -0.24659        0.867
            35          -0.24560        0.867
            36          -0.24464        0.867
            37          -0.24373        0.867
            38          -0.24286        0.868
            39          -0.24203        0.868
            40          -0.24123        0.868
            41          -0.24046        0.868
            42          -0.23973        0.869
            43          -0.23902        0.869
            44          -0.23833        0.869
            45          -0.23768        0.869
            46          -0.23704        0.869
            47          -0.23643        0.870
            48          -0.23584        0.870
            49          -0.23527        0.870
            50          -0.23472        0.870
            51          -0.23418        0.870
            52          -0.23366        0.870
            53          -0.23316        0.870
            54          -0.23268        0.870
            55          -0.23221        0.870
            56          -0.23175        0.870
            57          -0.23130        0.870
            58          -0.23087        0.871
            59          -0.23045        0.871
            60          -0.23004        0.871
            61          -0.22965        0.871
            62          -0.22926        0.871
            63          -0.22888        0.871
            64          -0.22852        0.871
            65          -0.22816        0.871
            66          -0.22781        0.871
            67          -0.22747        0.871
            68          -0.22714        0.871
            69          -0.22682        0.871
            70          -0.22650        0.872
            71          -0.22620        0.872
            72          -0.22590        0.872
            73          -0.22560        0.872
            74          -0.22531        0.872
            75          -0.22503        0.872
            76          -0.22476        0.872
            77          -0.22449        0.872
            78          -0.22423        0.872
            79          -0.22397        0.872
            80          -0.22372        0.872
            81          -0.22347        0.872
            82          -0.22323        0.872
            83          -0.22299        0.872
            84          -0.22276        0.872
            85          -0.22253        0.872
            86          -0.22231        0.872
            87          -0.22209        0.872
            88          -0.22188        0.872
            89          -0.22167        0.873
            90          -0.22146        0.873
            91          -0.22126        0.873
            92          -0.22106        0.873
            93          -0.22087        0.873
            94          -0.22067        0.873
            95          -0.22049        0.873
            96          -0.22030        0.873
            97          -0.22012        0.873
            98          -0.21994        0.873
            99          -0.21977        0.873
         Final          -0.21959        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147088>  1392   8189 |
  a |   2387 <10597>  3528 |
the |  12650   1588 <46609>|
----+----------------------+
(row = reference; col = test)

6_train_classifier_accuracy: 0.872946826875
6_train_correction: 32002
6_train_golden_correction: 4921
6_train_tp: 3159
6_train_fp: 28843
6_train_fn: 1762
6_train_precision: 0.0987125804637
6_train_recall: 0.641942694574
6_train_f1: 0.171112856485

  18.158 next-word-2=='barker' and label is 'the'
  18.143 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  16.322 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
 -15.556 first-tag=='PRP' and label is 'the'
  15.513 head-word=='approach*' and label is 'a'
  15.303 head-word=='distance13' and label is 'a'
  14.660 head-word=='cattles' and label is 'a'
  14.578 head-word=='metres' and label is 'a'
  13.787 head-word=='35km/h' and label is 'a'
  12.753 head-word=='radius' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<16107>  239  1329 |
  a |   391  <885>  521 |
the |  1773   273 <4305>|
----+-------------------+
(row = reference; col = test)

6_test_classifier_accuracy: 0.824729891957
6_test_correction: 4706
6_test_golden_correction: 495
6_test_tp: 300
6_test_fp: 4406
6_test_fn: 195
6_test_precision: 0.0637484062898
6_test_recall: 0.606060606061
6_test_f1: 0.115362430302

--------------------------------------------------
                    Round 7
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.258
             2          -0.48909        0.688
             3          -0.43632        0.779
             4          -0.40009        0.813
             5          -0.37444        0.826
             6          -0.35533        0.834
             7          -0.34050        0.840
             8          -0.32862        0.844
             9          -0.31886        0.847
            10          -0.31069        0.850
            11          -0.30374        0.852
            12          -0.29773        0.854
            13          -0.29248        0.855
            14          -0.28785        0.856
            15          -0.28373        0.858
            16          -0.28004        0.859
            17          -0.27671        0.860
            18          -0.27368        0.861
            19          -0.27091        0.861
            20          -0.26838        0.862
            21          -0.26604        0.863
            22          -0.26388        0.863
            23          -0.26188        0.863
            24          -0.26001        0.864
            25          -0.25827        0.864
            26          -0.25664        0.865
            27          -0.25511        0.865
            28          -0.25367        0.865
            29          -0.25231        0.866
            30          -0.25102        0.866
            31          -0.24981        0.867
            32          -0.24865        0.867
            33          -0.24756        0.867
            34          -0.24651        0.867
            35          -0.24552        0.868
            36          -0.24457        0.868
            37          -0.24367        0.868
            38          -0.24280        0.868
            39          -0.24197        0.869
            40          -0.24117        0.869
            41          -0.24041        0.869
            42          -0.23967        0.869
            43          -0.23897        0.869
            44          -0.23829        0.869
            45          -0.23763        0.870
            46          -0.23700        0.870
            47          -0.23639        0.870
            48          -0.23580        0.870
            49          -0.23523        0.870
            50          -0.23468        0.870
            51          -0.23415        0.870
            52          -0.23364        0.870
            53          -0.23314        0.871
            54          -0.23265        0.871
            55          -0.23218        0.871
            56          -0.23173        0.871
            57          -0.23129        0.871
            58          -0.23086        0.871
            59          -0.23044        0.871
            60          -0.23003        0.871
            61          -0.22964        0.871
            62          -0.22925        0.871
            63          -0.22888        0.872
            64          -0.22851        0.872
            65          -0.22816        0.872
            66          -0.22781        0.872
            67          -0.22747        0.872
            68          -0.22714        0.872
            69          -0.22682        0.872
            70          -0.22651        0.872
            71          -0.22620        0.872
            72          -0.22590        0.872
            73          -0.22561        0.872
            74          -0.22532        0.872
            75          -0.22504        0.872
            76          -0.22477        0.872
            77          -0.22450        0.872
            78          -0.22424        0.872
            79          -0.22398        0.872
            80          -0.22373        0.873
            81          -0.22349        0.873
            82          -0.22325        0.873
            83          -0.22301        0.873
            84          -0.22278        0.873
            85          -0.22255        0.873
            86          -0.22233        0.873
            87          -0.22211        0.873
            88          -0.22190        0.873
            89          -0.22169        0.873
            90          -0.22148        0.873
            91          -0.22128        0.873
            92          -0.22108        0.873
            93          -0.22089        0.873
            94          -0.22070        0.873
            95          -0.22051        0.873
            96          -0.22033        0.873
            97          -0.22014        0.873
            98          -0.21997        0.873
            99          -0.21979        0.873
         Final          -0.21962        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147196>  1350   8082 |
  a |   2414 <10624>  3455 |
the |  12633   1597 <46053>|
----+----------------------+
(row = reference; col = test)

7_train_classifier_accuracy: 0.873476889856
7_train_correction: 31801
7_train_golden_correction: 4931
7_train_tp: 3175
7_train_fp: 28626
7_train_fn: 1756
7_train_precision: 0.0998396276847
7_train_recall: 0.643885621578
7_train_f1: 0.172873788522

  17.615 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  17.496 next-word-2=='barker' and label is 'the'
  15.385 head-word=='distance13' and label is 'a'
 -15.351 first-tag=='PRP' and label is 'the'
  15.348 head-word=='approach*' and label is 'a'
  15.252 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  14.785 head-word=='cattles' and label is 'a'
  14.568 head-word=='metres' and label is 'a'
  14.463 head-word=='crossroads' and label is 'a'
  13.334 next-word-2=='//www.rlhleagueofnurses.org.uk/hospital_history/medical_developments/me' and label is 'the'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<16113>  252  1351 |
  a |   403  <902>  511 |
the |  1879   278 <4758>|
----+-------------------+
(row = reference; col = test)

7_test_classifier_accuracy: 0.823269179869
7_test_correction: 4812
7_test_golden_correction: 485
7_test_tp: 267
7_test_fp: 4545
7_test_fn: 218
7_test_precision: 0.0554862842893
7_test_recall: 0.550515463918
7_test_f1: 0.100811780253

--------------------------------------------------
                    Round 8
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.48956        0.688
             3          -0.43652        0.778
             4          -0.40023        0.813
             5          -0.37456        0.827
             6          -0.35545        0.835
             7          -0.34061        0.840
             8          -0.32873        0.844
             9          -0.31897        0.847
            10          -0.31080        0.850
            11          -0.30384        0.852
            12          -0.29783        0.854
            13          -0.29258        0.855
            14          -0.28794        0.856
            15          -0.28382        0.857
            16          -0.28013        0.859
            17          -0.27679        0.860
            18          -0.27376        0.860
            19          -0.27100        0.861
            20          -0.26846        0.862
            21          -0.26612        0.862
            22          -0.26396        0.863
            23          -0.26196        0.863
            24          -0.26009        0.864
            25          -0.25835        0.864
            26          -0.25671        0.865
            27          -0.25518        0.865
            28          -0.25374        0.865
            29          -0.25238        0.866
            30          -0.25109        0.866
            31          -0.24987        0.866
            32          -0.24872        0.866
            33          -0.24762        0.867
            34          -0.24658        0.867
            35          -0.24558        0.867
            36          -0.24463        0.867
            37          -0.24372        0.867
            38          -0.24285        0.868
            39          -0.24202        0.868
            40          -0.24122        0.868
            41          -0.24046        0.868
            42          -0.23972        0.868
            43          -0.23902        0.868
            44          -0.23833        0.869
            45          -0.23768        0.869
            46          -0.23704        0.869
            47          -0.23643        0.869
            48          -0.23584        0.869
            49          -0.23527        0.869
            50          -0.23472        0.869
            51          -0.23419        0.869
            52          -0.23367        0.870
            53          -0.23317        0.870
            54          -0.23268        0.870
            55          -0.23221        0.870
            56          -0.23175        0.870
            57          -0.23131        0.870
            58          -0.23088        0.870
            59          -0.23046        0.871
            60          -0.23005        0.871
            61          -0.22966        0.871
            62          -0.22927        0.871
            63          -0.22889        0.871
            64          -0.22853        0.871
            65          -0.22817        0.871
            66          -0.22782        0.871
            67          -0.22748        0.871
            68          -0.22715        0.871
            69          -0.22683        0.871
            70          -0.22651        0.871
            71          -0.22621        0.872
            72          -0.22590        0.872
            73          -0.22561        0.872
            74          -0.22532        0.872
            75          -0.22504        0.872
            76          -0.22477        0.872
            77          -0.22450        0.872
            78          -0.22424        0.872
            79          -0.22398        0.872
            80          -0.22373        0.872
            81          -0.22348        0.872
            82          -0.22324        0.872
            83          -0.22300        0.872
            84          -0.22277        0.872
            85          -0.22254        0.872
            86          -0.22232        0.872
            87          -0.22210        0.872
            88          -0.22188        0.873
            89          -0.22167        0.873
            90          -0.22147        0.873
            91          -0.22126        0.873
            92          -0.22106        0.873
            93          -0.22087        0.873
            94          -0.22068        0.873
            95          -0.22049        0.873
            96          -0.22030        0.873
            97          -0.22012        0.873
            98          -0.21994        0.873
            99          -0.21977        0.873
         Final          -0.21959        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147369>  1379   8121 |
  a |   2431 <10604>  3455 |
the |  12717   1580 <46253>|
----+----------------------+
(row = reference; col = test)

8_train_classifier_accuracy: 0.873100222736
8_train_correction: 31896
8_train_golden_correction: 4838
8_train_tp: 3113
8_train_fp: 28783
8_train_fn: 1725
8_train_precision: 0.0975984449461
8_train_recall: 0.643447705663
8_train_f1: 0.16948875701

  18.160 head-word=='destinations' and label is 'a'
  17.490 next-word-2=='//www.egovmonitor.com/node/28185' and label is 'a'
  17.030 next-word-2=='barker' and label is 'the'
  15.891 head-word=='approach*' and label is 'a'
  15.536 head-word=='distance13' and label is 'a'
  15.359 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  14.732 head-word=='cattles' and label is 'a'
  14.525 head-word=='crossroads' and label is 'a'
 -14.448 first-tag=='PRP' and label is 'the'
  14.207 head-word=='35km/h' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15886>  246  1343 |
  a |   404  <912>  503 |
the |  1822   280 <4546>|
----+-------------------+
(row = reference; col = test)

8_test_classifier_accuracy: 0.822758461183
8_test_correction: 4809
8_test_golden_correction: 578
8_test_tp: 342
8_test_fp: 4467
8_test_fn: 236
8_test_precision: 0.0711166562695
8_test_recall: 0.59169550173
8_test_f1: 0.12697234082

--------------------------------------------------
                    Round 9
--------------------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.09861        0.259
             2          -0.48933        0.688
             3          -0.43656        0.778
             4          -0.40034        0.813
             5          -0.37467        0.827
             6          -0.35553        0.835
             7          -0.34067        0.840
             8          -0.32877        0.844
             9          -0.31899        0.847
            10          -0.31079        0.850
            11          -0.30381        0.852
            12          -0.29778        0.854
            13          -0.29252        0.855
            14          -0.28787        0.857
            15          -0.28374        0.858
            16          -0.28003        0.859
            17          -0.27669        0.860
            18          -0.27365        0.861
            19          -0.27088        0.861
            20          -0.26834        0.862
            21          -0.26600        0.863
            22          -0.26384        0.863
            23          -0.26183        0.864
            24          -0.25996        0.864
            25          -0.25821        0.864
            26          -0.25658        0.865
            27          -0.25505        0.865
            28          -0.25360        0.865
            29          -0.25224        0.866
            30          -0.25095        0.866
            31          -0.24974        0.866
            32          -0.24858        0.867
            33          -0.24748        0.867
            34          -0.24644        0.867
            35          -0.24545        0.867
            36          -0.24450        0.867
            37          -0.24359        0.868
            38          -0.24272        0.868
            39          -0.24189        0.868
            40          -0.24109        0.868
            41          -0.24033        0.868
            42          -0.23959        0.868
            43          -0.23889        0.869
            44          -0.23821        0.869
            45          -0.23755        0.869
            46          -0.23692        0.869
            47          -0.23631        0.869
            48          -0.23572        0.869
            49          -0.23515        0.869
            50          -0.23460        0.870
            51          -0.23407        0.870
            52          -0.23355        0.870
            53          -0.23306        0.870
            54          -0.23257        0.870
            55          -0.23210        0.870
            56          -0.23165        0.870
            57          -0.23120        0.870
            58          -0.23077        0.871
            59          -0.23036        0.871
            60          -0.22995        0.871
            61          -0.22956        0.871
            62          -0.22917        0.871
            63          -0.22880        0.871
            64          -0.22843        0.871
            65          -0.22808        0.871
            66          -0.22773        0.871
            67          -0.22739        0.871
            68          -0.22706        0.871
            69          -0.22674        0.871
            70          -0.22643        0.871
            71          -0.22612        0.872
            72          -0.22582        0.872
            73          -0.22553        0.872
            74          -0.22524        0.872
            75          -0.22497        0.872
            76          -0.22469        0.872
            77          -0.22442        0.872
            78          -0.22416        0.872
            79          -0.22391        0.872
            80          -0.22366        0.872
            81          -0.22341        0.872
            82          -0.22317        0.872
            83          -0.22294        0.872
            84          -0.22271        0.872
            85          -0.22248        0.872
            86          -0.22226        0.872
            87          -0.22204        0.872
            88          -0.22183        0.872
            89          -0.22162        0.872
            90          -0.22141        0.872
            91          -0.22121        0.872
            92          -0.22101        0.872
            93          -0.22082        0.872
            94          -0.22063        0.873
            95          -0.22044        0.873
            96          -0.22026        0.873
            97          -0.22008        0.873
            98          -0.21990        0.873
            99          -0.21973        0.873
         Final          -0.21956        0.873
    |                    t |
    |                    h |
    |             a      e |
----+----------------------+
    |<147684>  1372   8239 |
  a |   2427 <10602>  3477 |
the |  12726   1583 <46327>|
----+----------------------+
(row = reference; col = test)

9_train_classifier_accuracy: 0.872784586051
9_train_correction: 32090
9_train_golden_correction: 4931
9_train_tp: 3187
9_train_fp: 28903
9_train_fn: 1744
9_train_precision: 0.0993144281708
9_train_recall: 0.646319205029
9_train_f1: 0.172172550714

  17.073 next-word-2=='barker' and label is 'the'
  15.251 next-word-2=='//wisdom.unu.edu/en/ageing-' and label is 'the'
  14.857 head-word=='approach*' and label is 'a'
  14.806 head-word=='cattles' and label is 'a'
 -14.590 first-tag=='PRP' and label is 'the'
  14.516 head-word=='crossroads' and label is 'a'
  14.398 head-word=='metres' and label is 'a'
  13.886 head-word=='35km/h' and label is 'a'
  13.347 next-word-2=='//www.rlhleagueofnurses.org.uk/hospital_history/medical_developments/me' and label is 'the'
  12.962 head-word=='radius' and label is 'a'

    |                 t |
    |                 h |
    |           a     e |
----+-------------------+
    |<15499>  256  1294 |
  a |   411  <866>  526 |
the |  1816   286 <4460>|
----+-------------------+
(row = reference; col = test)

9_test_classifier_accuracy: 0.819430235303
9_test_correction: 4743
9_test_golden_correction: 485
9_test_tp: 265
9_test_fp: 4478
9_test_fn: 220
9_test_precision: 0.05587181109
9_test_recall: 0.546391752577
9_test_f1: 0.101377199694

--------------------------------------------------
                    Summary
--------------------------------------------------
train_classifier_accuracy_se: 0.00013045995557
train_correction_se: 43.616370525
train_golden_correction_se: 18.6852288661
train_tp_se: 9.32624731009
train_fp_se: 38.0647985531
train_fn_se: 10.7232354156
train_precision_se: 0.000226442050635
train_recall_se: 0.00103868026786
train_f1_se: 0.000334210299853

test_classifier_accuracy_se: 0.00133993890072
test_correction_se: 43.3533799784
test_golden_correction_se: 18.6852288661
test_tp_se: 9.48800412217
test_fp_se: 38.3191857951
test_fn_se: 10.7870292481
test_precision_se: 0.0016909131527
test_recall_se: 0.00777723935224
test_f1_se: 0.00277851797936

train_classifier_accuracy: 0.873381139931
train_correction: 31872.1
train_golden_correction: 4874.4
train_tp: 3151.3
train_fp: 28720.8
train_fn: 1723.1
train_precision: 0.0988725837062
train_recall: 0.64652494324
train_f1: 0.171513750993

test_classifier_accuracy: 0.820028587068
test_correction: 4842.6
test_golden_correction: 541.6
test_tp: 303.0
test_fp: 4539.6
test_fn: 238.6
test_precision: 0.0625188458301
test_recall: 0.560402381131
test_f1: 0.112419328535

end_time: 2016/11/20-23:15:21
